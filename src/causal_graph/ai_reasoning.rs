//! AI-powered Causal Reasoning Engine
//!
//! This module provides AI-driven causal graph generation using LLMs
//! to replace or augment statistical approaches.

use crate::oracle_agent::LlmClient;
use crate::oracle_agent::LlmProvider;
use crate::causal_graph::types::{
    CausalGraph, CausalNode, CausalEdge, CausalPath, NodeType, EdgeType, PathType
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, anyhow};
use log::{info, debug, warn};

/// Configuration for AI causal reasoning
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIReasoningConfig {
    /// LLM provider to use
    pub llm_provider: LlmProvider,
    
    /// Model name
    pub model: String,
    
    /// Temperature for LLM generation (0.0-2.0)
    pub temperature: f32,
    
    /// Maximum tokens for response
    pub max_tokens: u32,
    
    /// Enable JSON mode for structured output
    pub enable_json_mode: bool,
    
    /// Minimum number of nodes to generate
    pub min_nodes: usize,
    
    /// Maximum number of nodes to generate
    pub max_nodes: usize,
    
    /// Minimum number of causal paths
    pub min_paths: usize,
    
    /// Maximum number of causal paths
    pub max_paths: usize,
}

impl Default for AIReasoningConfig {
    fn default() -> Self {
        Self {
            llm_provider: LlmProvider::DeepSeek,
            model: "deepseek-chat".to_string(),
            temperature: 0.7,
            max_tokens: 2000,
            enable_json_mode: true,
            min_nodes: 3,
            max_nodes: 5,
            min_paths: 2,
            max_paths: 3,
        }
    }
}

/// AI-generated causal node response
#[derive(Debug, Clone, Serialize, Deserialize)]
struct AINode {
    /// Node ID
    pub id: String,
    /// Node name
    pub name: String,
    /// Node type
    pub node_type: String,
    /// Importance score (0.0-1.0)
    pub importance: f64,
    /// Whether this can be intervened upon
    #[serde(default)]
    pub intervention_target: bool,
}

/// AI-generated causal edge response
#[derive(Debug, Clone, Serialize, Deserialize)]
struct AIEdge {
    /// Edge ID
    pub id: String,
    /// Source node ID
    pub source: String,
    /// Target node ID
    pub target: String,
    /// Causal strength (0.0-1.0)
    pub weight: f64,
    /// Edge type
    pub edge_type: String,
}

/// AI-generated causal path response
#[derive(Debug, Clone, Serialize, Deserialize)]
struct AIPath {
    /// Path ID
    pub id: String,
    /// Sequence of node IDs
    pub nodes: Vec<String>,
    /// Path strength
    pub strength: f64,
    /// Path type
    pub path_type: String,
}

/// AI-generated complete causal graph response
#[derive(Debug, Clone, Serialize, Deserialize)]
struct AICausalResponse {
    /// List of causal nodes
    pub nodes: Vec<AINode>,
    /// List of causal edges
    pub edges: Vec<AIEdge>,
    /// List of main causal paths
    pub paths: Vec<AIPath>,
    /// Explanation/reasoning from AI
    #[serde(default)]
    pub reasoning: String,
    /// Confidence score (0.0-1.0)
    #[serde(default)]
    pub confidence: f64,
}

/// Prompt template with variable placeholders
#[derive(Debug, Clone)]
pub struct PromptTemplate {
    /// Template content
    pub content: String,
}

impl PromptTemplate {
    /// Create a new prompt template
    pub fn new(content: &str) -> Self {
        Self {
            content: content.to_string(),
        }
    }
    
    /// Replace variables in the template
    pub fn render(&self, variables: &HashMap<String, String>) -> String {
        let mut content = self.content.clone();
        for (key, value) in variables {
            content = content.replace(&format!("{{{{{}}}}}", key), value);
        }
        content
    }
}

/// AI Causal Reasoning Engine
pub struct AIReasoningEngine {
    /// LLM client
    llm_client: LlmClient,
    /// Configuration
    config: AIReasoningConfig,
}

impl AIReasoningEngine {
    /// Create a new AI reasoning engine
    pub fn new(config: AIReasoningConfig) -> Result<Self> {
        let mut llm_config = match config.llm_provider {
            LlmProvider::OpenAI => {
                crate::oracle_agent::LlmClientConfig::openai(&config.model)
                    .with_temperature(config.temperature)
                    .with_max_tokens(config.max_tokens)
            },
            LlmProvider::Anthropic => {
                crate::oracle_agent::LlmClientConfig::anthropic(&config.model)
                    .with_temperature(config.temperature)
                    .with_max_tokens(config.max_tokens)
            },
            LlmProvider::DeepSeek => {
                crate::oracle_agent::LlmClientConfig::deepseek(&config.model)
                    .with_temperature(config.temperature)
                    .with_max_tokens(config.max_tokens)
            },
            LlmProvider::Minimax => {
                crate::oracle_agent::LlmClientConfig::minimax(&config.model)
                    .with_temperature(config.temperature)
                    .with_max_tokens(config.max_tokens)
            },
            LlmProvider::Local => {
                crate::oracle_agent::LlmClientConfig::local(
                    "http://localhost:11434/api/generate",
                    &config.model
                )
                .with_temperature(config.temperature)
                .with_max_tokens(config.max_tokens)
            },
        };
        
        // å¦‚æœå¯ç”¨ JSON æ¨¡å¼ï¼Œé…ç½®å®¢æˆ·ç«¯å¼ºåˆ¶è¿”å› JSON
        if config.enable_json_mode {
            llm_config = llm_config.with_json_mode();
        }
        
        let llm_client = LlmClient::new(llm_config)?;
        
        info!("âœ… AIæ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ: {:?} ({})", 
              config.llm_provider, config.model);
        
        Ok(Self {
            llm_client,
            config,
        })
    }
    
    /// Create engine from existing LLM client
    pub fn from_client(llm_client: LlmClient, config: AIReasoningConfig) -> Self {
        Self {
            llm_client,
            config,
        }
    }
    
    /// Generate a causal graph from a prompt
    pub async fn generate_causal_graph(&self, prompt: &str, context: &str) -> Result<CausalGraph> {
        info!("ğŸ¤– å¼€å§‹AIå› æœå›¾ç”Ÿæˆ...");
        
        // Build the full prompt
        let full_prompt = self.build_causal_analysis_prompt(prompt, context)?;
        
        // Call LLM
        let response = self.llm_client.generate_response(&full_prompt).await?;
        
        debug!("LLMå“åº”: {}", response.text);
        
        // Parse response
        let ai_response = self.parse_ai_response(&response.text)?;
        
        // Validate response
        self.validate_ai_response(&ai_response)?;
        
        // Convert to CausalGraph
        let mut graph = self.convert_to_causal_graph(&ai_response)?;
        
        // Final validation
        if !graph.is_valid() {
            warn!("âš ï¸ AIç”Ÿæˆçš„å› æœå›¾æœªé€šè¿‡éªŒè¯ï¼Œå°è¯•ä¿®å¤...");
            self.attempt_graph_fix(&mut graph)?;
        }
        
        info!("âœ… AIå› æœå›¾ç”Ÿæˆå®Œæˆï¼Œç½®ä¿¡åº¦: {:.2}", ai_response.confidence);
        
        Ok(graph)
    }
    
    /// Build the causal analysis prompt
    fn build_causal_analysis_prompt(&self, user_prompt: &str, context: &str) -> Result<String> {
        let template = PromptTemplate::new(include_str!("prompts/causal_analysis.txt"));
        
        let mut variables = HashMap::new();
        variables.insert("SCENARIO".to_string(), user_prompt.to_string());
        variables.insert("CONTEXT".to_string(), context.to_string());
        variables.insert("MIN_NODES".to_string(), self.config.min_nodes.to_string());
        variables.insert("MAX_NODES".to_string(), self.config.max_nodes.to_string());
        variables.insert("MIN_PATHS".to_string(), self.config.min_paths.to_string());
        variables.insert("MAX_PATHS".to_string(), self.config.max_paths.to_string());
        
        Ok(template.render(&variables))
    }
    
    /// Parse AI response into structured format
    fn parse_ai_response(&self, response: &str) -> Result<AICausalResponse> {
        let response_trimmed = response.trim();
        
        // å°è¯•1: ç›´æ¥è§£æJSON
        match serde_json::from_str::<AICausalResponse>(response_trimmed) {
            Ok(parsed) => {
                eprintln!("âœ… ç›´æ¥è§£æJSONæˆåŠŸ");
                return Ok(parsed);
            }
            Err(_) => {
                // ç›´æ¥è§£æå¤±è´¥æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºå“åº”å¯èƒ½æ˜¯markdownæ ¼å¼ï¼Œä¸éœ€è¦è¾“å‡ºè­¦å‘Š
                // eprintln!("âš ï¸ ç›´æ¥è§£æå¤±è´¥: {}", e);
            }
        }
        
        // å°è¯•2: ä»markdownä¸­æå–JSON
        if let Some(json_data) = Self::extract_json_from_markdown(response_trimmed) {
            eprintln!("ğŸ“ ä»markdownæå–åˆ°JSONæ•°æ®ï¼Œé•¿åº¦: {}", json_data.len());
            
            match serde_json::from_str::<AICausalResponse>(&json_data) {
                Ok(parsed) => {
                    eprintln!("âœ… Markdownæå–çš„JSONè§£ææˆåŠŸ");
                    return Ok(parsed);
                }
                Err(e) => {
                    eprintln!("âš ï¸ Markdownæå–çš„JSONè§£æå¤±è´¥: {}", e);
                    
                    // å°è¯•ä¿®å¤
                    if let Some(fixed) = Self::fix_truncated_json(&json_data) {
                        match serde_json::from_str::<AICausalResponse>(&fixed) {
                            Ok(parsed) => {
                                eprintln!("âœ… ä¿®å¤åçš„JSONè§£ææˆåŠŸ");
                                return Ok(parsed);
                            }
                            Err(e) => {
                                eprintln!("âš ï¸ ä¿®å¤åçš„JSONè§£æå¤±è´¥: {}", e);
                            }
                        }
                    }
                }
            }
        } else {
            eprintln!("âš ï¸ æ— æ³•ä»markdownæå–JSONæ•°æ®");
        }
        
        // å°è¯•3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}
        if let Some(start) = response_trimmed.find('{') {
            if let Some(end) = response_trimmed.rfind('}') {
                if end > start {
                    let json_str = &response_trimmed[start..=end];
                    eprintln!("ğŸ“ å°è¯•è§£æä» {{ åˆ° }} çš„å†…å®¹ï¼Œé•¿åº¦: {}", json_str.len());
                    
                    match serde_json::from_str::<AICausalResponse>(json_str) {
                        Ok(parsed) => {
                            eprintln!("âœ… ä»{{}}æå–çš„JSONè§£ææˆåŠŸ");
                            return Ok(parsed);
                        }
                        Err(e) => {
                            eprintln!("âš ï¸ ä»{{}}æå–çš„JSONè§£æå¤±è´¥: {}", e);
                            
                            // å°è¯•ä¿®å¤
                            if let Some(fixed) = Self::fix_truncated_json(json_str) {
                                match serde_json::from_str::<AICausalResponse>(&fixed) {
                                    Ok(parsed) => {
                                        eprintln!("âœ… ä¿®å¤åçš„{{}} JSONè§£ææˆåŠŸ");
                                        return Ok(parsed);
                                    }
                                    Err(e) => {
                                        eprintln!("âš ï¸ ä¿®å¤åçš„{{}} JSONè§£æå¤±è´¥: {}", e);
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        
        // æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        Err(anyhow!("æ— æ³•è§£æAIå“åº”ä¸ºJSONæ ¼å¼"))
    }
    
    /// ä¿®å¤æˆªæ–­æˆ–ä¸å®Œæ•´çš„JSON
    fn fix_truncated_json(json_str: &str) -> Option<String> {
        let mut fixed = json_str.to_string();
        
        // è®¡ç®—æ‹¬å·å¹³è¡¡
        let open_braces = fixed.matches('{').count();
        let close_braces = fixed.matches('}').count();
        let open_brackets = fixed.matches('[').count();
        let close_brackets = fixed.matches(']').count();
        
        // è¡¥å…¨ç¼ºå¤±çš„é—­åˆæ‹¬å·
        for _ in 0..(open_braces - close_braces) {
            fixed.push('}');
        }
        for _ in 0..(open_brackets - close_brackets) {
            fixed.push(']');
        }
        
        // ä¿®å¤å°¾éšé€—å·
        fixed = fixed.replace(",}", "}").replace(",]", "]");
        
        // å¦‚æœJSONçœ‹èµ·æ¥å®Œæ•´ï¼Œå°è¯•è§£æ
        if fixed.parse::<serde_json::Value>().is_ok() {
            return Some(fixed);
        }
        
        None
    }
    
    /// ä»markdownæ ¼å¼ä¸­æå–JSONæ•°æ®
    fn extract_json_from_markdown(response: &str) -> Option<String> {
        // ç­–ç•¥1: æŸ¥æ‰¾ "### è¯¦ç»†æ•°æ®" æˆ– "JSON" æ ‡è®°åçš„ä»£ç å—
        let markers = ["### è¯¦ç»†æ•°æ®", "### JSON", "è¯¦ç»†æ•°æ®", "JSONæ•°æ®"];
        
        for marker in &markers {
            if let Some(pos) = response.find(marker) {
                let after_marker = &response[pos + marker.len()..];
                // æŸ¥æ‰¾æ¥ä¸‹æ¥çš„ ```json æˆ– ``` ä»£ç å—
                if let Some(code_start) = after_marker.find("```json") {
                    let after_code = &after_marker[code_start + 7..];
                    if let Some(code_end) = after_code.find("```") {
                        return Some(after_code[..code_end].trim().to_string());
                    }
                } else if let Some(code_start) = after_marker.find("```") {
                    let after_code = &after_marker[code_start + 3..];
                    if let Some(code_end) = after_code.find("```") {
                        return Some(after_code[..code_end].trim().to_string());
                    }
                }
            }
        }
        
        // ç­–ç•¥2: æŸ¥æ‰¾æ‰€æœ‰ ```json ä»£ç å—ï¼ˆè¿”å›æœ€å¤§çš„ä¸€ä¸ªï¼Œé€šå¸¸æ˜¯å®Œæ•´æ•°æ®ï¼‰
        let mut best_json: Option<String> = None;
        let mut search_start = 0;
        
        while let Some(code_start) = response[search_start..].find("```json") {
            let actual_start = search_start + code_start + 7;
            if let Some(code_end) = response[actual_start..].find("```") {
                let json_content = response[actual_start..actual_start + code_end].trim();
                // é€‰æ‹©æœ€é•¿çš„æœ‰æ•ˆJSON
                if json_content.starts_with('{') && json_content.len() > best_json.as_ref().map_or(0, |s| s.len()) {
                    best_json = Some(json_content.to_string());
                }
                search_start = actual_start + code_end + 3;
            } else {
                // æ‰¾åˆ°äº†å¼€å§‹ä½†æ²¡æœ‰ç»“æŸï¼Œå¯èƒ½æ˜¯æˆªæ–­çš„JSON
                let partial_json = response[actual_start..].trim();
                if partial_json.starts_with('{') && partial_json.len() > best_json.as_ref().map_or(0, |s| s.len()) {
                    best_json = Some(partial_json.to_string());
                }
                break;
            }
        }
        
        if best_json.is_some() {
            return best_json;
        }
        
        // ç­–ç•¥3: æŸ¥æ‰¾ ``` ä»£ç å—ï¼ˆå¯èƒ½æ˜¯çº¯JSONï¼‰
        if let Some(start) = response.find("```") {
            let after_start = &response[start + 3..];
            // è·³è¿‡å¯èƒ½çš„è¯­è¨€æ ‡è¯†ç¬¦ï¼ˆå¦‚ jsonï¼‰
            let content_start = if after_start.starts_with("json") { 4 } else { 0 };
            let after_lang = &after_start[content_start..];
            
            if let Some(end) = after_lang.find("```") {
                let json_content = after_lang[..end].trim();
                if json_content.starts_with('{') {
                    return Some(json_content.to_string());
                }
            }
        }
        
        None
    }
    
    /// Validate AI-generated response
    fn validate_ai_response(&self, response: &AICausalResponse) -> Result<()> {
        // Check node count
        if response.nodes.len() < self.config.min_nodes || 
           response.nodes.len() > self.config.max_nodes {
            return Err(anyhow!(
                "èŠ‚ç‚¹æ•°é‡ä¸ç¬¦åˆè¦æ±‚: {} (è¦æ±‚: {}-{})",
                response.nodes.len(),
                self.config.min_nodes,
                self.config.max_nodes
            ));
        }
        
        // Check path count
        if response.paths.len() < self.config.min_paths || 
           response.paths.len() > self.config.max_paths {
            return Err(anyhow!(
                "å› æœè·¯å¾„æ•°é‡ä¸ç¬¦åˆè¦æ±‚: {} (è¦æ±‚: {}-{})",
                response.paths.len(),
                self.config.min_paths,
                self.config.max_paths
            ));
        }
        
        // Check for at least one treatment and outcome
        let has_treatment = response.nodes.iter().any(|n| 
            n.node_type.to_lowercase().contains("treatment"));
        let has_outcome = response.nodes.iter().any(|n| 
            n.node_type.to_lowercase().contains("outcome"));
        
        if !has_treatment || !has_outcome {
            return Err(anyhow!("å› æœå›¾å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªtreatmentå’Œä¸€ä¸ªoutcomeèŠ‚ç‚¹"));
        }
        
        // Check confidence
        if response.confidence < 0.5 {
            warn!("âš ï¸ AIç½®ä¿¡åº¦è¾ƒä½: {:.2}", response.confidence);
        }
        
        Ok(())
    }
    
    /// Convert AI response to CausalGraph
    fn convert_to_causal_graph(&self, response: &AICausalResponse) -> Result<CausalGraph> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let mut graph = CausalGraph {
            id: format!("ai_graph_{}", now),
            nodes: Vec::new(),
            edges: Vec::new(),
            main_paths: Vec::new(),
            metadata: crate::causal_graph::types::GraphMetadata {
                created_at: now,
                updated_at: now,
                num_core_variables: response.nodes.len(),
                num_main_paths: response.paths.len(),
                version: "1.0.0".to_string(),
            },
        };
        
        // Convert nodes
        for ai_node in &response.nodes {
            let node_type = self.parse_node_type(&ai_node.node_type)?;
            let node = CausalNode {
                id: ai_node.id.clone(),
                name: ai_node.name.clone(),
                node_type,
                value: None,
                intervention_target: ai_node.intervention_target,
                importance: ai_node.importance.clamp(0.0, 1.0),
            };
            graph.add_node(node).map_err(|e| anyhow::anyhow!(e))?;
        }
        
        // Convert edges
        for ai_edge in &response.edges {
            let edge_type = self.parse_edge_type(&ai_edge.edge_type)?;
            let edge = CausalEdge {
                id: ai_edge.id.clone(),
                source: ai_edge.source.clone(),
                target: ai_edge.target.clone(),
                weight: ai_edge.weight.clamp(-1.0, 1.0),
                edge_type,
            };
            graph.add_edge(edge).map_err(|e| anyhow::anyhow!(e))?;
        }
        
        // Convert paths
        for ai_path in &response.paths {
            let path_type = self.parse_path_type(&ai_path.path_type)?;
            let path = CausalPath {
                id: ai_path.id.clone(),
                nodes: ai_path.nodes.clone(),
                strength: ai_path.strength.clamp(0.0, 1.0),
                path_type,
            };
            graph.main_paths.push(path);
        }
        
        Ok(graph)
    }
    
    /// Parse node type from string
    fn parse_node_type(&self, type_str: &str) -> Result<NodeType> {
        let type_lower = type_str.to_lowercase();
        match type_lower.as_str() {
            "treatment" | "å¹²é¢„" => Ok(NodeType::Treatment),
            "outcome" | "ç»“æœ" => Ok(NodeType::Outcome),
            "confounder" | "æ··æ·†å› å­" => Ok(NodeType::Confounder),
            "mediator" | "ä¸­ä»‹å› å­" => Ok(NodeType::Mediator),
            "control" | "æ§åˆ¶å˜é‡" => Ok(NodeType::Control),
            _ => {
                warn!("æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {}, é»˜è®¤ä¸ºControl", type_str);
                Ok(NodeType::Control)
            }
        }
    }
    
    /// Parse edge type from string
    fn parse_edge_type(&self, type_str: &str) -> Result<EdgeType> {
        let type_lower = type_str.to_lowercase();
        match type_lower.as_str() {
            "direct" | "ç›´æ¥" => Ok(EdgeType::Direct),
            "indirect" | "é—´æ¥" => Ok(EdgeType::Indirect),
            "confounding" | "æ··æ·†" => Ok(EdgeType::Confounding),
            _ => {
                warn!("æœªçŸ¥è¾¹ç±»å‹: {}, é»˜è®¤ä¸ºIndirect", type_str);
                Ok(EdgeType::Indirect)
            }
        }
    }
    
    /// Parse path type from string
    fn parse_path_type(&self, type_str: &str) -> Result<PathType> {
        let type_lower = type_str.to_lowercase();
        match type_lower.as_str() {
            "frontdoor" | "å‰é—¨" => Ok(PathType::FrontDoor),
            "backdoor" | "åé—¨" => Ok(PathType::BackDoor),
            "confounded" | "æ··æ·†" => Ok(PathType::Confounded),
            _ => {
                warn!("æœªçŸ¥è·¯å¾„ç±»å‹: {}, é»˜è®¤ä¸ºFrontDoor", type_str);
                Ok(PathType::FrontDoor)
            }
        }
    }
    
    /// Attempt to fix invalid graph
    fn attempt_graph_fix(&self, graph: &mut CausalGraph) -> Result<()> {
        // Fix node count
        if graph.nodes.len() < self.config.min_nodes {
            warn!("èŠ‚ç‚¹ä¸è¶³ï¼Œæ·»åŠ é»˜è®¤èŠ‚ç‚¹");
            for i in graph.nodes.len()..self.config.min_nodes {
                let node = CausalNode {
                    id: format!("N{}", i),
                    name: format!("Variable_{}", i),
                    node_type: NodeType::Control,
                    value: None,
                    intervention_target: false,
                    importance: 0.5,
                };
                let _ = graph.add_node(node);
            }
        } else if graph.nodes.len() > self.config.max_nodes {
            warn!("èŠ‚ç‚¹è¿‡å¤šï¼Œç§»é™¤é¢å¤–èŠ‚ç‚¹");
            graph.nodes.truncate(self.config.max_nodes);
            graph.metadata.num_core_variables = graph.nodes.len();
        }
        
        // Fix path count
        if graph.main_paths.len() < self.config.min_paths {
            warn!("è·¯å¾„ä¸è¶³ï¼Œæ·»åŠ é»˜è®¤è·¯å¾„");
            while graph.main_paths.len() < self.config.min_paths && graph.nodes.len() >= 2 {
                let path = CausalPath {
                    id: format!("path_{}", graph.main_paths.len()),
                    nodes: vec![
                        graph.nodes[0].id.clone(),
                        graph.nodes[1].id.clone()
                    ],
                    strength: 0.5,
                    path_type: PathType::FrontDoor,
                };
                graph.main_paths.push(path);
            }
            graph.metadata.num_main_paths = graph.main_paths.len();
        }
        
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_prompt_template() {
        let template = PromptTemplate::new("Hello {{NAME}}, today is {{DAY}}");
        let mut vars = HashMap::new();
        vars.insert("NAME".to_string(), "Alice".to_string());
        vars.insert("DAY".to_string(), "Monday".to_string());
        
        let result = template.render(&vars);
        assert_eq!(result, "Hello Alice, today is Monday");
    }
    
    #[test]
    fn test_node_type_parsing() {
        let config = AIReasoningConfig::default();
        let engine = AIReasoningEngine {
            llm_client: unsafe { std::mem::zeroed() },  // For testing only
            config,
        };
        
        assert!(matches!(engine.parse_node_type("treatment").unwrap(), NodeType::Treatment));
        assert!(matches!(engine.parse_node_type("outcome").unwrap(), NodeType::Outcome));
        assert!(matches!(engine.parse_node_type("confounder").unwrap(), NodeType::Confounder));
    }
    
    #[test]
    fn test_edge_type_parsing() {
        let config = AIReasoningConfig::default();
        let engine = AIReasoningEngine {
            llm_client: unsafe { std::mem::zeroed() },  // For testing only
            config,
        };
        
        assert!(matches!(engine.parse_edge_type("direct").unwrap(), EdgeType::Direct));
        assert!(matches!(engine.parse_edge_type("indirect").unwrap(), EdgeType::Indirect));
        assert!(matches!(engine.parse_edge_type("confounding").unwrap(), EdgeType::Confounding));
    }
}
