//! LLM API å®¢æˆ·ç«¯æ¨¡å—
//!
//! æ”¯æŒè°ƒç”¨ OpenAIã€Claude ç­‰ LLM API æ¥è·å–æ™ºèƒ½ä½“å“åº”
//!
//! ä½¿ç”¨æ–¹æ³•ï¼š
//!   1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
//!      export OPENAI_API_KEY=your_key
//!      export ANTHROPIC_API_KEY=your_key
//!      export DEEPSEEK_API_KEY=your_key
//!
//!   2. åˆ›å»ºå®¢æˆ·ç«¯ï¼š
//!      let client = LlmClient::new(Provider::OpenAI)?;
//!
//!   3. è°ƒç”¨ APIï¼š
//!      let response = client.generate_response(prompt).await?;

use anyhow::{Result, anyhow};
use serde::{Serialize, Deserialize};
use serde_json::Value;
use std::env;
use reqwest::Client;
use std::time::Duration;
use log::{info, warn, debug};

/// LLM API æä¾›å•†
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum LlmProvider {
    /// OpenAI (GPT-4, GPT-3.5, etc.)
    OpenAI,
    /// Anthropic (Claude)
    Anthropic,
    /// DeepSeek (deepseek-chat, deepseek-coder, etc.)
    DeepSeek,
    /// Minimax (minimax-chat)
    Minimax,
    /// æœ¬åœ° LLM (é€šè¿‡ HTTP API)
    Local,
}

/// LLM å®¢æˆ·ç«¯é…ç½®
#[derive(Debug, Clone)]
pub struct LlmClientConfig {
    /// API æä¾›å•†
    pub provider: LlmProvider,
    /// æ¨¡å‹åç§°
    pub model: String,
    /// API å¯†é’¥
    pub api_key: Option<String>,
    /// API ç«¯ç‚¹
    pub api_endpoint: String,
    /// æœ€å¤§é‡è¯•æ¬¡æ•°
    pub max_retries: u32,
    /// è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub timeout_secs: u64,
    /// æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰
    pub temperature: f32,
    /// æœ€å¤§ token æ•°
    pub max_tokens: u32,
    /// å¼ºåˆ¶ JSON è¾“å‡ºæ¨¡å¼ï¼ˆä»…éƒ¨åˆ†æä¾›å•†æ”¯æŒï¼Œå¦‚ OpenAIã€DeepSeekï¼‰
    pub response_format_json: bool,
}

impl Default for LlmClientConfig {
    fn default() -> Self {
        Self {
            provider: LlmProvider::OpenAI,
            model: "gpt-3.5-turbo".to_string(),
            api_key: None,
            api_endpoint: "https://api.openai.com/v1/chat/completions".to_string(),
            max_retries: 3,
            timeout_secs: 30,
            temperature: 0.7,
            max_tokens: 500,
            response_format_json: false,
        }
    }
}

impl LlmClientConfig {
    /// åˆ›å»º OpenAI é…ç½®
    pub fn openai(model: &str) -> Self {
        let mut config = Self::default();
        config.provider = LlmProvider::OpenAI;
        config.model = model.to_string();
        config.api_endpoint = "https://api.openai.com/v1/chat/completions".to_string();
        
        // ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥
        if let Ok(key) = env::var("OPENAI_API_KEY") {
            config.api_key = Some(key);
        }
        
        config
    }
    
    /// åˆ›å»º Anthropic (Claude) é…ç½®
    pub fn anthropic(model: &str) -> Self {
        let mut config = Self::default();
        config.provider = LlmProvider::Anthropic;
        config.model = model.to_string();
        config.api_endpoint = "https://api.anthropic.com/v1/messages".to_string();
        
        // ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥
        if let Ok(key) = env::var("ANTHROPIC_API_KEY") {
            config.api_key = Some(key);
        }
        
        config
    }
    
    /// åˆ›å»ºæœ¬åœ° LLM é…ç½®
    pub fn local(endpoint: &str, model: &str) -> Self {
        let mut config = Self::default();
        config.provider = LlmProvider::Local;
        config.model = model.to_string();
        config.api_endpoint = endpoint.to_string();
        config
    }

    /// åˆ›å»º DeepSeek é…ç½®
    pub fn deepseek(model: &str) -> Self {
        let mut config = Self::default();
        config.provider = LlmProvider::DeepSeek;
        config.model = model.to_string();
        config.api_endpoint = "https://api.deepseek.com/v1/chat/completions".to_string();

        // ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥
        if let Ok(key) = env::var("DEEPSEEK_API_KEY") {
            config.api_key = Some(key);
        }

        config
    }

    /// åˆ›å»º Minimax é…ç½®
    pub fn minimax(model: &str) -> Self {
        let mut config = Self::default();
        config.provider = LlmProvider::Minimax;
        config.model = model.to_string();
        config.api_endpoint = "https://api.minimax.chat/v1/text/chatcompletion_v2".to_string();

        // ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥
        if let Ok(key) = env::var("Minimax_API_KEY") {
            config.api_key = Some(key);
        }

        config
    }

    /// è®¾ç½® API å¯†é’¥
    pub fn with_api_key(mut self, api_key: &str) -> Self {
        self.api_key = Some(api_key.to_string());
        self
    }
    
    /// è®¾ç½®æ¸©åº¦
    pub fn with_temperature(mut self, temperature: f32) -> Self {
        self.temperature = temperature.clamp(0.0, 2.0);
        self
    }
    
    /// è®¾ç½®æœ€å¤§ token æ•°
    pub fn with_max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = max_tokens;
        self
    }

    /// å¯ç”¨ JSON è¾“å‡ºæ¨¡å¼ï¼ˆå¼ºåˆ¶è¿”å› JSON æ ¼å¼ï¼‰
    pub fn with_json_mode(mut self) -> Self {
        self.response_format_json = true;
        self
    }
}

/// LLM å“åº”
#[derive(Debug, Clone, Deserialize)]
pub struct LlmResponse {
    /// å“åº”æ–‡æœ¬
    pub text: String,
    /// ä½¿ç”¨çš„ token æ•°
    pub usage: Usage,
    /// æ¨¡å‹åç§°
    pub model: String,
    /// å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub response_time_ms: u64,
}

/// Token ä½¿ç”¨æƒ…å†µ
#[derive(Debug, Clone, Deserialize)]
pub struct Usage {
    /// è¾“å…¥ token æ•°
    pub prompt_tokens: u32,
    /// è¾“å‡º token æ•°
    pub completion_tokens: u32,
    /// æ€» token æ•°
    pub total_tokens: u32,
}

/// LLM å®¢æˆ·ç«¯
#[derive(Clone)]
pub struct LlmClient {
    config: LlmClientConfig,
    http_client: Client,
}

impl LlmClient {
    /// åˆ›å»ºæ–°çš„ LLM å®¢æˆ·ç«¯
    pub fn new(config: LlmClientConfig) -> Result<Self> {
        // éªŒè¯é…ç½®
        if config.api_key.is_none() && matches!(config.provider, LlmProvider::OpenAI | LlmProvider::Anthropic) {
            warn!("âš ï¸ æœªé…ç½® API å¯†é’¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨");
        }
        
        let http_client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_secs))
            .user_agent("MultiAgentOracle/1.0")
            .build()
            .map_err(|e| anyhow!("åˆ›å»º HTTP å®¢æˆ·ç«¯å¤±è´¥: {}", e))?;
        
        info!("ğŸ¤– åˆ›å»º LLM å®¢æˆ·ç«¯: {:?}, æ¨¡å‹: {}", config.provider, config.model);
        
        Ok(Self {
            config,
            http_client,
        })
    }
    
    /// ç”Ÿæˆå“åº”
    pub async fn generate_response(&self, prompt: &str) -> Result<LlmResponse> {
        let start_time = std::time::Instant::now();

        debug!("å‘é€è¯·æ±‚åˆ° LLM: {}...", &prompt[..prompt.len().min(100)]);

        let response_text = match self.config.provider {
            LlmProvider::OpenAI => self.call_openai(prompt).await?,
            LlmProvider::Anthropic => self.call_anthropic(prompt).await?,
            LlmProvider::DeepSeek => self.call_deepseek(prompt).await?,
            LlmProvider::Minimax => self.call_minimax(prompt).await?,
            LlmProvider::Local => self.call_local(prompt).await?,
        };

        let response_time = start_time.elapsed().as_millis() as u64;

        info!("âœ… LLM å“åº”å®Œæˆï¼Œè€—æ—¶: {}ms, é•¿åº¦: {} å­—ç¬¦", response_time, response_text.len());

        Ok(LlmResponse {
            text: response_text,
            usage: Usage {
                prompt_tokens: 0,
                completion_tokens: 0,
                total_tokens: 0,
            },
            model: self.config.model.clone(),
            response_time_ms: response_time,
        })
    }
    
    /// è°ƒç”¨ OpenAI API
    async fn call_openai(&self, prompt: &str) -> Result<String> {
        let api_key = self.config.api_key.as_ref()
            .ok_or_else(|| anyhow!("æœªé…ç½® OpenAI API å¯†é’¥"))?;

        let request_body = serde_json::json!({
            "model": self.config.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        });

        let response = self.http_client
            .post(&self.config.api_endpoint)
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| anyhow!("å‘é€ OpenAI è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("OpenAI API é”™è¯¯: {} - {}", status, error_text));
        }

        let json: Value = response.json().await
            .map_err(|e| anyhow!("è§£æ OpenAI å“åº”å¤±è´¥: {}", e))?;

        let text = json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| anyhow!("æ— æ³•ä» OpenAI å“åº”ä¸­æå–æ–‡æœ¬"))?;

        Ok(text.to_string())
    }
    
    /// è°ƒç”¨ Anthropic (Claude) API
    async fn call_anthropic(&self, prompt: &str) -> Result<String> {
        let api_key = self.config.api_key.as_ref()
            .ok_or_else(|| anyhow!("æœªé…ç½® Anthropic API å¯†é’¥"))?;

        let request_body = serde_json::json!({
            "model": self.config.model,
            "max_tokens": self.config.max_tokens,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        });

        let response = self.http_client
            .post(&self.config.api_endpoint)
            .header("x-api-key", api_key)
            .header("anthropic-version", "2023-06-01")
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| anyhow!("å‘é€ Anthropic è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("Anthropic API é”™è¯¯: {} - {}", status, error_text));
        }

        let json: Value = response.json().await
            .map_err(|e| anyhow!("è§£æ Anthropic å“åº”å¤±è´¥: {}", e))?;

        let text = json["content"][0]["text"]
            .as_str()
            .ok_or_else(|| anyhow!("æ— æ³•ä» Anthropic å“åº”ä¸­æå–æ–‡æœ¬"))?;

        Ok(text.to_string())
    }
    
    /// è°ƒç”¨æœ¬åœ° LLM API
    async fn call_local(&self, prompt: &str) -> Result<String> {
        let request_body = serde_json::json!({
            "model": self.config.model,
            "prompt": prompt,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        });

        let response = self.http_client
            .post(&self.config.api_endpoint)
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| anyhow!("å‘é€æœ¬åœ° LLM è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("æœ¬åœ° LLM API é”™è¯¯: {} - {}", status, error_text));
        }

        let json: Value = response.json().await
            .map_err(|e| anyhow!("è§£ææœ¬åœ° LLM å“åº”å¤±è´¥: {}", e))?;

        // å°è¯•å¤šç§å¯èƒ½çš„å“åº”æ ¼å¼
        let text = json["response"]
            .as_str()
            .or_else(|| json["output"].as_str())
            .or_else(|| json["text"].as_str())
            .or_else(|| json["completion"].as_str())
            .ok_or_else(|| anyhow!("æ— æ³•ä»æœ¬åœ° LLM å“åº”ä¸­æå–æ–‡æœ¬"))?;

        Ok(text.to_string())
    }
    
    /// æ£€æŸ¥ API å¯†é’¥æ˜¯å¦é…ç½®
    pub fn has_api_key(&self) -> bool {
        self.config.api_key.is_some()
    }

    /// è°ƒç”¨ DeepSeek API (å…¼å®¹ OpenAI æ ¼å¼)
    async fn call_deepseek(&self, prompt: &str) -> Result<String> {
        let api_key = self.config.api_key.as_ref()
            .ok_or_else(|| anyhow!("æœªé…ç½® DeepSeek API å¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY"))?;

        let request_body = serde_json::json!({
            "model": self.config.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        });

        let response = self.http_client
            .post(&self.config.api_endpoint)
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| anyhow!("å‘é€ DeepSeek è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("DeepSeek API é”™è¯¯: {} - {}", status, error_text));
        }

        let json: Value = response.json().await
            .map_err(|e| anyhow!("è§£æ DeepSeek å“åº”å¤±è´¥: {}", e))?;

        let text = json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| anyhow!("æ— æ³•ä» DeepSeek å“åº”ä¸­æå–æ–‡æœ¬"))?;

        Ok(text.to_string())
    }

    /// è°ƒç”¨ Minimax API
    async fn call_minimax(&self, prompt: &str) -> Result<String> {
        let api_key = self.config.api_key.as_ref()
            .ok_or_else(|| anyhow!("æœªé…ç½® Minimax API å¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ Minimax_API_KEY"))?;

        // Minimax API å‚æ•°è¯´æ˜ï¼š
        // - tokens_to_generate: æœ€å¤§è¾“å‡ºtokenæ•°
        // - max_tokens: ä¹Ÿæ˜¯æœ‰æ•ˆçš„å‚æ•°åï¼ˆæŸäº›æ¨¡å‹ä½¿ç”¨ï¼‰
        let max_tokens_val = std::cmp::max(self.config.max_tokens, 4000); // ç¡®ä¿è‡³å°‘4000 tokensï¼Œé¿å…JSONæˆªæ–­

        let mut request_body = serde_json::json!({
            "model": self.config.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": self.config.temperature,
            "tokens_to_generate": max_tokens_val,
            "max_tokens": max_tokens_val,  // Minimax å¯èƒ½ä½¿ç”¨ä¸åŒçš„å‚æ•°å
        });

        // å¦‚æœå¯ç”¨äº† JSON æ¨¡å¼ï¼Œæ·»åŠ æç¤ºï¼ˆMinimax ä¸æ”¯æŒ response_format å‚æ•°ï¼‰
        if self.config.response_format_json {
            // Minimax ä¸æ”¯æŒ OpenAI é£æ ¼çš„ response_format å‚æ•°
            // éœ€è¦åœ¨ prompt ä¸­æ˜ç¡®è¦æ±‚ JSON æ ¼å¼
        }

        let response = self.http_client
            .post(&self.config.api_endpoint)
            .header("Authorization", format!("Bearer {}", api_key))
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| anyhow!("å‘é€ Minimax è¯·æ±‚å¤±è´¥: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow!("Minimax API é”™è¯¯: {} - {}", status, error_text));
        }

        let json: Value = response.json().await
            .map_err(|e| anyhow!("è§£æ Minimax å“åº”å¤±è´¥: {}", e))?;

        // Minimax å“åº”æ ¼å¼: {"choices": [{"message": {"content": "..."}}], "usage": {...}}
        let text = json["choices"][0]["message"]["content"]
            .as_str()
            .ok_or_else(|| anyhow!("æ— æ³•ä» Minimax å“åº”ä¸­æå–æ–‡æœ¬"))?;

        Ok(text.to_string())
    }

    /// è·å–æä¾›å•†ä¿¡æ¯
    pub fn get_provider_info(&self) -> String {
        format!("{:?} ({})", self.config.provider, self.config.model)
    }
}
